{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of semantic-based taxonomies\n",
    "\n",
    "The following notebook explores the use of semantic embeddings to create taxonomies of entities, with the goal of creating an ontology of the ARIA dataset. It leverages embeddings from the tags identified in articles extracted from OpenAlex, and uses them to cluster entities into groups. This taxonomy is assumed hierarchical.\n",
    "\n",
    "Multiple options are explored to create the taxonomy, including:\n",
    "- **Strict hierarchical clustering**: Iteratively cluster the entity embeddings using Agglomerative Clustering and KMeans. At each level, clustering is performed on the subsets that were created at the previous level. This allows for strict hierarchical entity breakdowns. \n",
    "- **Strict hierarchical clustering with varying number of clusters**: Iteratively cluster the entity embeddings using KMeans. At each level, clustering is performed on the subsets that were created at the previous level. The number of clusters is proportional to the parent cluster size.\n",
    "- **Fuzzy hierarchical clustering**: Cluster the entity embeddings using an array of methods, including KMeans, Agglomerative Clustering, DBSCAN, OPTICS, and HDBSCAN. Several levels of resolution are used, and the taxonomy is built by concatenating these. This allows for non-strict hierarchical entity breakdowns.\n",
    "- **Hierarchical clustering using a dendrogram climb algorithm**: Reconstruct the dendrogram of the Agglomerative Clustering, and use it to create the taxonomy.\n",
    "- **Hierarchical clustering using the centroids of parent-level clusters**: Cluster the entity embeddings using KMeans, and use the centroids of the clusters as nodes in subsequent levels of the taxonomy.\n",
    "- **Meta clustering using the co-occurrence of terms in the set of clustering results**: Use outputs from all previous clustering methods to create a tag co-occurrence matrix. Apply community detection algorithms to this matrix to create the taxonomy. \n",
    "\n",
    "The utils for this notebook include all necessary functions to create the taxonomy, including class ClusteringRoutine that performs any of the clustering methods described above. The function run_clustering_generators is used to run all clustering methods and return the results in a dictionary. The function make_dataframe is used to create a dataframe with the results of the clustering methods. The function make_plots is used to create a series of plots to visualize the results of the clustering methods. The function make_cooccurrences is used to create a co-occurrence matrix of the clustering results. The function make_subplot_embeddings is used to create a series of subplots with the embeddings of the entities in the taxonomy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from IPython.display import display\n",
    "import boto3, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from itertools import product\n",
    "from toolz import pipe\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from dap_aria_mapping import PROJECT_DIR, BUCKET_NAME, logger\n",
    "from dap_aria_mapping.pipeline.semantic_taxonomy.utils import (\n",
    "    make_subplot_embeddings,\n",
    "    make_dataframe,\n",
    "    make_plots,\n",
    "    make_cooccurrences,\n",
    "    run_clustering_generators\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain entities from OpenAlex\n",
    "The entity tags are obtained from OpenAlex. Filtering is applied to remove entities that are too frequent or too infrequent. The entities are then embedded using the SPECTER model. In addition, two-dimensional representations of the embeddings are obtained using UMAP, for plotting purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    try:\n",
    "        logger.info(\"Downloading embeddings from S3\")\n",
    "        embeddings_object = s3.get_object(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Key=\"outputs/embeddings/embeddings.pkl\"\n",
    "        )\n",
    "        embeddings = pickle.loads(embeddings_object[\"Body\"].read())\n",
    "    except:\n",
    "        logger.info(\"Failed to download from S3. Attempting to load from local instead\")\n",
    "        with open(f\"{PROJECT_DIR}/outputs/embeddings.pkl\", \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "except:\n",
    "    logger.info(\"Failed to load embeddings. Running pipeline with default (test) parameters\")\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        f\"python {PROJECT_DIR}/dap_aria_mapping/pipeline/embeddings/local_export.py\", \n",
    "        shell=True\n",
    "    )\n",
    "    with open(f\"{PROJECT_DIR}/outputs/embeddings.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "Consider possible combinations of UMAP parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    [\"n_neighbors\", [20]],\n",
    "    [\"min_dist\", [0.1]],\n",
    "    [\"n_components\", [8]],\n",
    "]\n",
    "\n",
    "keys, permuts = ([x[0] for x in params], list(product(*[x[1] for x in params])))\n",
    "param_perms = [{k: v for k, v in zip(keys, perm)} for perm in permuts]\n",
    "\n",
    "for perm in param_perms:\n",
    "    embeddings_2d = umap.UMAP(**perm).fit_transform(embeddings)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=1)\n",
    "    fig.suptitle(f\"{perm}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strictly hierarchical clustering\n",
    "The following clustering routine iteratively clusters the entity embeddings using Agglomerative Clustering and KMeans. At each level, clustering is performed on the subsets that were created at the previous level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_configs = [\n",
    "    [\n",
    "        KMeans,\n",
    "        [\n",
    "            {\"n_clusters\": 5, \"n_init\": 5},  # parent level\n",
    "            {\"n_clusters\": 20, \"n_init\": 5}, # nested level 1\n",
    "            {\"n_clusters\": 20, \"n_init\": 5}, # nested level 2\n",
    "            {\"n_clusters\": 40, \"n_init\": 5}  # nested level 3\n",
    "        ],\n",
    "    ],\n",
    "    [\n",
    "        AgglomerativeClustering,\n",
    "        [\n",
    "            {\"n_clusters\": 5}, # parent level\n",
    "            {\"n_clusters\": 20}, # nested level 1\n",
    "            {\"n_clusters\": 20}, # nested level 2\n",
    "            {\"n_clusters\": 40}  # nested level 3\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "# run clustering generators\n",
    "cluster_outputs_s, plot_dicts = run_clustering_generators(cluster_configs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, axis = plt.subplots(2, 4, figsize=(32, 16), dpi=200)\n",
    "for idx, (cdict, cluster) in enumerate(plot_dicts):\n",
    "    _, lvl = divmod(idx, 4)\n",
    "    make_subplot_embeddings(\n",
    "        embeddings=embeddings_2d,\n",
    "        clabels=[int(e) for e in cdict.values()],\n",
    "        axis=axis.flat[idx],\n",
    "        label=f\"{cluster[-1]} {str(lvl)}\",\n",
    "        s=4,\n",
    "    )\n",
    "fig.savefig(PROJECT_DIR / \"outputs\" / \"figures\" / \"semantic_taxonomy\" / \"clustering_strict.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print silhouettes\n",
    "for output in cluster_outputs_s:\n",
    "    print(\n",
    "        \"Silhouette score - {} clusters - {}: {}\".format(\n",
    "            output[\"model\"][-1].__module__,\n",
    "            output[\"model\"][-1].get_params()[\"n_clusters\"],\n",
    "            output[\"silhouette\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strict hierarchical clustering with imbalanced nested clusters\n",
    "The following clustering routine iteratively clusters the entity embeddings using KMeans. At each level, clustering is performed on the subsets that were created at the previous level. The number of clusters at each level is allowed to vary, being determined by the size of the parent cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_configs = [\n",
    "    [\n",
    "        KMeans,\n",
    "        [\n",
    "            {\"n_clusters\": 5, \"n_init\": 5}, # parent level\n",
    "            {\"n_clusters\": 20, \"n_init\": 5},# nested level 1, total n_clusters is 20+\n",
    "            {\"n_clusters\": 20, \"n_init\": 5},# nested level 2, total n_clusters is 20+\n",
    "            {\"n_clusters\": 40, \"n_init\": 5},# nested level 2, total n_clusters is 40+\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "\n",
    "# run clustering generators with imbalanced nested clusters\n",
    "cluster_outputs_simb, plot_dicts = run_clustering_generators(cluster_configs, embeddings, imbalanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, axis = plt.subplots(1, 4, figsize=(32, 8), dpi=200)\n",
    "for idx, (cdict, cluster) in enumerate(plot_dicts):\n",
    "    labels = [int(e) for e in cdict.values()]\n",
    "    di = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "    labels = [di[label] for label in labels]\n",
    "    _, lvl = divmod(idx, 3)\n",
    "    make_subplot_embeddings(\n",
    "        embeddings=embeddings_2d,\n",
    "        clabels=labels,\n",
    "        axis=axis.flat[idx],\n",
    "        label=f\"{cluster[-1]} {str(lvl)}\",\n",
    "        s=4,\n",
    "    )\n",
    "fig.savefig(PROJECT_DIR / \"outputs\" / \"figures\" / \"semantic_taxonomy\" / \"clustering_strict_imb.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print silhouettes\n",
    "for output in cluster_outputs_simb:\n",
    "    print(\n",
    "        \"Silhouette score - {} clusters - {}: {}\".format(\n",
    "            output[\"model\"][-1].__module__,\n",
    "            output[\"model\"][-1].get_params()[\"n_clusters\"],\n",
    "            output[\"silhouette\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy hierarchical clustering\n",
    "The following approach iteratively clusters the entity embeddings using any sklearn method that supports the `predict_proba` method. No notion of level exists through this approach: more fine-grained clusterings are agnostic about the parent cluster output. Including several lists of parameter values will produce outputs for the Cartesian product of all parameter values within a clustering method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m cluster_configs \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     [KMeans, [{\u001b[39m\"\u001b[39m\u001b[39mn_clusters\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m5\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m40\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mn_init\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m5\u001b[39m}]], \u001b[39m# level 1, level 2, level 3, level 4\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     [AgglomerativeClustering, [{\u001b[39m\"\u001b[39m\u001b[39mn_clusters\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m5\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m40\u001b[39m]}]], \u001b[39m# level 1, level 2, level 3, level 4\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     [DBSCAN, [{\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0.15\u001b[39m, \u001b[39m0.25\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m8\u001b[39m, \u001b[39m16\u001b[39m]}]], \u001b[39m# level 1, level 2, level 3, level 4\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     [HDBSCAN, [{\u001b[39m\"\u001b[39m\u001b[39mmin_cluster_size\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m4\u001b[39m, \u001b[39m8\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m8\u001b[39m, \u001b[39m16\u001b[39m]}]], \u001b[39m# level 1, level 2, level 3, level 4\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[39m# run clustering generators with fuzzy clusters\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cluster_outputs_f_, plot_dicts \u001b[39m=\u001b[39m run_clustering_generators(cluster_configs, embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_configs = [\n",
    "    [KMeans, [{\"n_clusters\": [5, 20, 20, 40], \"n_init\": 5}]], # level 1, level 2, level 3, level 4\n",
    "    [AgglomerativeClustering, [{\"n_clusters\": [5, 20, 20, 40]}]], # level 1, level 2, level 3, level 4\n",
    "    [DBSCAN, [{\"eps\": [0.15, 0.25], \"min_samples\": [8, 16]}]], # level 1, level 2, level 3, level 4\n",
    "    [HDBSCAN, [{\"min_cluster_size\": [4, 8], \"min_samples\": [8, 16]}]], # level 1, level 2, level 3, level 4\n",
    "]\n",
    "\n",
    "# run clustering generators with fuzzy clusters\n",
    "cluster_outputs_f_, plot_dicts = run_clustering_generators(cluster_configs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [HACK] - fix this. For exports, I create a single dictionary for the fuzzy clusters\n",
    "cluster_outputs_f = []\n",
    "for group in [\"sklearn.cluster._kmeans\", \"sklearn.cluster._agglomerative\"]:\n",
    "    dict_group = {\n",
    "        \"labels\": defaultdict(list),\n",
    "        \"model\": [],\n",
    "        \"silhouette\": [],\n",
    "        \"centroid_params\": None\n",
    "    }\n",
    "\n",
    "    cluster_outpu = [x for x in cluster_outputs_f_ if x[\"model\"][0].__module__ == group]\n",
    "    for clust in cluster_outpu:\n",
    "        for k, v in clust[\"labels\"].items():\n",
    "            dict_group[\"labels\"][k].append(v[0])\n",
    "        dict_group[\"model\"].append(\"_\".join([clust[\"model\"][0].__module__.replace(\".\", \"\"), str(clust[\"model\"][0].get_params()[\"n_clusters\"])]))\n",
    "        dict_group[\"silhouette\"].append(clust[\"silhouette\"][0])\n",
    "    cluster_outputs_f.append(dict_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, axis = plt.subplots(4, 4, figsize=(40, 40), dpi=200)\n",
    "for idx, (cdict, cluster) in enumerate(plot_dicts):\n",
    "    make_subplot_embeddings(\n",
    "        embeddings=embeddings_2d,\n",
    "        clabels=[int(e) for e in cdict.values()],\n",
    "        axis=axis.flat[idx],\n",
    "        label=f\"{cluster[-1].__module__}\",\n",
    "        cmap=\"gist_ncar\",\n",
    "    )\n",
    "fig.savefig(PROJECT_DIR / \"outputs\" / \"figures\" / \"semantic_taxonomy\" / \"clustering_fuzzy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print silhouettes\n",
    "for cluster in cluster_outputs_f_:\n",
    "    print(\n",
    "        \"Silhouette score - {}: {}\".format(\n",
    "            cluster[\"model\"][-1], cluster[\"silhouette\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dendrograms from Agglomerative Clustering (enforces hierarchy)\n",
    "This approach uses a single run of any sklearn clustering method that supports a children_ attribute. The children_ attribute is used to recreate th dendrogram that produced the clustering, which is then used to create the taxonomy. The climbing algorithm advances one union of subtrees at a time. The number of levels is determined by the `dendrogram_levels` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_configs = [[AgglomerativeClustering, [{\"n_clusters\": 100}]]]\n",
    "\n",
    "# run clustering generators with dendrograms\n",
    "cluster_outputs_d, plot_dicts = run_clustering_generators(cluster_configs, embeddings, dendrogram_levels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, axis = plt.subplots(2, 3, figsize=(24, 16), dpi=200)\n",
    "for i, ax in zip(range(6), axis.flat):\n",
    "    make_subplot_embeddings(\n",
    "        embeddings=embeddings_2d,\n",
    "        clabels=[int(e[i]) for e in cluster_outputs_d[\"labels\"].values()],\n",
    "        axis=ax,\n",
    "        label=f\"denrogram - level {i}\",\n",
    "        s=4,\n",
    "    )\n",
    "fig.savefig(PROJECT_DIR / \"outputs\" / \"figures\" / \"semantic_taxonomy\" / \"clustering_dendrogram.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids of Kmeans clustering as children nodes for further clustering (à la [job skill taxonomy](https://github.com/nestauk/skills-taxonomy-v2/tree/dev/skills_taxonomy_v2/pipeline/skills_taxonomy))\n",
    "This approach uses any number of nested KMeans clustering runs. After a given level, the centroids of the previous level are used as the new data points for the next level. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_configs = [\n",
    "    [\n",
    "        KMeans,\n",
    "        [\n",
    "            {\"n_clusters\": 400, \"n_init\": 5, \"centroids\": False},\n",
    "            {\"n_clusters\": 200, \"n_init\": 5, \"centroids\": True},\n",
    "            {\"n_clusters\": 20, \"n_init\": 5, \"centroids\": True},\n",
    "            {\"n_clusters\": 5, \"n_init\": 5, \"centroids\": True},\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "\n",
    "# run clustering generators with centroids\n",
    "cluster_outputs_c, plot_dicts = run_clustering_generators(\n",
    "    cluster_configs, embeddings, embeddings_2d=embeddings_2d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [HACK] flip order, should be fixed in run_clustering_generators (should run highest level → lowest level)\n",
    "for output_dict in cluster_outputs_c:\n",
    "    for k,v in output_dict[\"labels\"].items():\n",
    "        output_dict[\"labels\"][k] = v[::-1]\n",
    "    output_dict[\"silhouette\"] = output_dict[\"silhouette\"][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, axis = plt.subplots(1, 4, figsize=(32, 8), dpi=200)\n",
    "for idx, cdict in enumerate(cluster_outputs_c):\n",
    "    if not cdict.get(\"centroid_params\", False):\n",
    "        axis[idx].scatter(\n",
    "            embeddings_2d[:, 0],\n",
    "            embeddings_2d[:, 1],\n",
    "            c=[e for e in cdict[\"labels\"].values()],\n",
    "            s=1,\n",
    "        )\n",
    "    else:\n",
    "        axis[idx].scatter(\n",
    "            cdict[\"centroid_params\"][\"n_embeddings_2d\"][:, 0],\n",
    "            cdict[\"centroid_params\"][\"n_embeddings_2d\"][:, 1],\n",
    "            c=cdict[\"model\"][idx].labels_,\n",
    "            s=cdict[\"centroid_params\"][\"sizes\"],\n",
    "        )\n",
    "    print(f\"Silhouette score ({idx}): {cdict['silhouette']}\")\n",
    "fig.savefig(PROJECT_DIR / \"outputs\" / \"figures\" / \"semantic_taxonomy\" / \"clustering_centroids.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "This section outputs silhouette scores for all relevant outputs above. It also constructs barplots of the cluster sizes for each level of the taxonomy across approaches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_kmeans_df = make_dataframe(cluster_outputs_s[3], \"_strict\")\n",
    "strict_agglom_df = make_dataframe(cluster_outputs_s[7], \"_strict\")\n",
    "strict_kmeans_imb_df = make_dataframe(cluster_outputs_simb[-1], \"_strict_imbalanced\")\n",
    "fuzzy_kmeans_df = make_dataframe(cluster_outputs_f[0], \"_fuzzy\")\n",
    "fuzzy_agglom_df = make_dataframe(cluster_outputs_f[1], \"_fuzzy\")\n",
    "dendrogram_df = make_dataframe(cluster_outputs_d, \"\")\n",
    "centroid_kmeans_df = make_dataframe(cluster_outputs_c[-1], \"_centroids\", cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strictly hierarchical clustering KMEANS\n",
    "make_plots(strict_kmeans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strictly hierarchical clustering Agglomerative\n",
    "make_plots(strict_agglom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strictly hierarchical clustering KMEANS with imbalanced cluster numbers\n",
    "make_plots(strict_kmeans_imb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy hierarchical clustering KMEANS\n",
    "make_plots(fuzzy_kmeans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram\n",
    "make_plots(dendrogram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids using Kmeans\n",
    "make_plots(centroid_kmeans_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = {\n",
    "    \"kmeans_strict\": cluster_outputs_s[3][\"silhouette\"],\n",
    "    \"agglom_strict\": cluster_outputs_s[7][\"silhouette\"],\n",
    "    \"kmeans_strict_imb\": cluster_outputs_simb[-1][\"silhouette\"],\n",
    "    \"kmeans_fuzzy\": cluster_outputs_f[0][\"silhouette\"],\n",
    "    \"agglom_fuzzy\": cluster_outputs_f[1][\"silhouette\"],\n",
    "    \"agglomerative_dendrogram\": cluster_outputs_d[\"silhouette\"],\n",
    "    \"kmeans_centroid\": cluster_outputs_c[-1][\"silhouette\"],\n",
    "}\n",
    "\n",
    "results = {\"_\".join([k,str(id)]): e for k,v in results.items() for id, e in enumerate(v)}\n",
    "\n",
    "silhouette_df = pd.DataFrame(results, index=[\"silhouette\"]).T.sort_values(\n",
    "    \"silhouette\", ascending=False\n",
    ")\n",
    "\n",
    "display(silhouette_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_aria_mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:35) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "416ac9d166dd1fbdb669b72a7a50d6c753a675aab1bc6f6f207f3a102e3085fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
